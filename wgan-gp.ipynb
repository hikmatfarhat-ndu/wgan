{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch import autograd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from networks import Generator,Discriminator\n",
    "from utils.utils import init_weight,random_sample\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(fake_images):\n",
    "        #n_samples = min(n_samples,self.cfg.batch_size)\n",
    "        #fake_images = self.generate_images(n_samples=n_samples)\n",
    "        grid = make_grid(fake_images, nrow=4, normalize=True)\n",
    "        img=recover_image(grid)\n",
    "        img.save(\"test.png\")\n",
    "        return grid\n",
    "    \n",
    "def recover_image(img):\n",
    "        # PIL expects the image to be of shape (H,W,C)\n",
    "        # in PyTorch it's (C,H,W)\n",
    "\n",
    "        img=img.cpu().numpy().transpose(1, 2,0)*255\n",
    "        return Image.fromarray(img.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class WGAN_GP():\n",
    "    \"\"\"\n",
    "    WGAN_GP a Wasserstein GAN with gradient penalty.\n",
    "    see https://arxiv.org/abs/1704.00028v3 \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "      \n",
    "        self.cfg = cfg\n",
    "        self.d_iter_per_g = 1 if self.cfg.d_iter_per_g is None else self.cfg.d_iter_per_g\n",
    "# generator and discriminator are described in the paper below\n",
    "# see https://arxiv.org/abs/1511.06434\n",
    "        self.generator = Generator(\n",
    "            z_dim=self.cfg.z_dim,\n",
    "            out_ch=self.cfg.img_ch,\n",
    "        )\n",
    "        self.discrim = Discriminator(self.cfg.img_ch)\n",
    "        self.initialize()\n",
    "        self.set_optimizers()\n",
    "\n",
    "    def initialize(self):\n",
    "        dir_list=os.listdir(self.cfg.weights_dir)\n",
    "       \n",
    "        if not dir_list:\n",
    "            self.generator.apply(init_weight)\n",
    "            self.discrim.apply(init_weight)\n",
    "        else:\n",
    "            gen_files=[f for f in dir_list if f.startswith(\"gen\")]\n",
    "            dis_files=[f for f in dir_list if f.startswith(\"dis\")]\n",
    "            gen_files.sort()\n",
    "            dis_files.sort()\n",
    "\n",
    "            self.generator.load_state_dict(torch.load(self.cfg.weights_dir+\"/\"+gen_files[-1]))\n",
    "            self.discrim.load_state_dict(torch.load(self.cfg.weights_dir+\"/\"+dis_files[-1]))\n",
    "            print(f\"loaded weights from {self.cfg.weights_dir}/{gen_files[-1]} and {self.cfg.weights_dir}/{dis_files[-1]}\")\n",
    "    def set_optimizers(self):\n",
    "        self.generator = self.generator.to(self.cfg.device)\n",
    "        self.discrim = self.discrim.to(self.cfg.device)\n",
    "\n",
    "        self.optG = Adam(self.generator.parameters(), lr=self.cfg.lr.g)\n",
    "        self.optD = Adam(self.discrim.parameters(), lr=self.cfg.lr.d)\n",
    "\n",
    "    def generator_step(self, data):\n",
    "        self.generator.train()\n",
    "        self.discrim.eval()\n",
    "\n",
    "        self.optG.zero_grad()\n",
    "\n",
    "        noise = random_sample(self.cfg.batch_size, self.cfg.z_dim, self.cfg.device)\n",
    "\n",
    "        fake_images = self.generator(noise)\n",
    "\n",
    "        fake_logits = self.discrim(fake_images)\n",
    "\n",
    "        loss = -fake_logits.mean().view(-1)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optG.step()\n",
    "\n",
    "        self.metrics[\"G-loss\"] += [loss.item()]\n",
    "\n",
    "    def discriminator_step(self, data):\n",
    "        self.generator.eval()\n",
    "        self.discrim.train()\n",
    "\n",
    "        self.optD.zero_grad()\n",
    "\n",
    "        real_images = data[0].float().to(self.cfg.device)\n",
    "\n",
    "        noise = random_sample(self.cfg.batch_size, self.cfg.z_dim, self.cfg.device)\n",
    "        fake_images = self.generator(noise)\n",
    "\n",
    "        real_logits = self.discrim(real_images)\n",
    "        fake_logits = self.discrim(fake_images)\n",
    "\n",
    "        gradient_penalty = self.cfg.w_gp * self._compute_gp(\n",
    "            real_images, fake_images\n",
    "        )\n",
    "\n",
    "        loss_c = fake_logits.mean() - real_logits.mean()\n",
    "\n",
    "        loss = loss_c + gradient_penalty\n",
    "\n",
    "        loss.backward()\n",
    "        self.optD.step()\n",
    "\n",
    "        self.metrics[\"D-loss\"] += [loss.item()]\n",
    "        self.metrics[\"GP\"] += [gradient_penalty.item()]\n",
    "    def train_epoch(self, dataloader):\n",
    "        # use of defaultdict instead of regular dict\n",
    "        # saves us the trouble of checking if a key exists\n",
    "        # which it doesn't when we start appending to it\n",
    "        self.metrics = defaultdict(list)\n",
    "        \n",
    "        loop = tqdm(dataloader, desc=\"Iteration: \",leave=False)\n",
    "\n",
    "        for idx, data in enumerate(loop):\n",
    "            self.discriminator_step(data)\n",
    "            if idx % self.cfg.d_iter_per_g == 0:\n",
    "                self.generator_step(data)\n",
    "\n",
    "\n",
    "    def _compute_gp(self, real_data, fake_data):\n",
    "        batch_size = real_data.size(0)\n",
    "        eps = torch.rand(batch_size, 1, 1, 1).to(real_data.device)\n",
    "        eps = eps.expand_as(real_data)\n",
    "        interpolation = eps * real_data + (1 - eps) * fake_data\n",
    "\n",
    "        interp_logits = self.discrim(interpolation)\n",
    "        grad_outputs = torch.ones_like(interp_logits)\n",
    "\n",
    "        gradients = autograd.grad(\n",
    "            outputs=interp_logits,\n",
    "            inputs=interpolation,\n",
    "            grad_outputs=grad_outputs,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "\n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        grad_norm = gradients.norm(2, 1)\n",
    "        return torch.mean((grad_norm - 1) ** 2)\n",
    "    \n",
    "    def generate_images(self, nsamples):\n",
    "        self.generator.eval()\n",
    "        with torch.no_grad():\n",
    "            noise = random_sample(self.cfg.batch_size, self.cfg.z_dim, self.cfg.device)[:nsamples]\n",
    "            fake_images = self.generator(noise)\n",
    "        return fake_images\n",
    "    \n",
    "    def save_model(self,epoch):\n",
    "        # if the directory doesn't exist, create it\n",
    "        try:\n",
    "            os.mkdir(self.cfg.weights_dir)\n",
    "        except:\n",
    "            pass\n",
    "        torch.save(self.generator.state_dict(), os.path.join(self.cfg.weights_dir, f\"generator_{epoch:03}.pth\"))\n",
    "        torch.save(self.discrim.state_dict(), os.path.join(self.cfg.weights_dir, f\"discrim_{epoch:03}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torchvision.transforms as vt\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "from munch import DefaultMunch\n",
    "from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "from models.wgan_gp import WGAN_GP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "cfg_path = \"config/config.yml\"\n",
    "with open(cfg_path, \"r\") as f:\n",
    "    print(f\"Loading config file: {cfg_path}\")\n",
    "    cfg = yaml.safe_load(f)\n",
    "cfg = DefaultMunch.fromDict(cfg)\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "transforms = vt.Compose([vt.ToTensor(),vt.Normalize(0.5, 0.5),\n",
    "    vt.Resize((cfg.imsize, cfg.imsize),antialias=True)])\n",
    "\n",
    "dataset = ImageFolder(\n",
    "    root=cfg.data_dir, transform=transforms\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    ")\n",
    "model=WGAN_GP(cfg)\n",
    "\n",
    "loop = trange(cfg.epochs, desc=\"Epoch: \", ncols=75)\n",
    "\n",
    "for epoch in loop:\n",
    "    model.train_epoch(dataloader)\n",
    "    \n",
    "    if (epoch+1) % cfg.save_model_freq == 0:\n",
    "        model.save_model(epoch)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
